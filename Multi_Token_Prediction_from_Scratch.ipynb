{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Token Prediction (MTP)"
      ],
      "metadata": {
        "id": "GZps_56evbje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0: Load Packages"
      ],
      "metadata": {
        "id": "ymOhz91jvhK8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaCHMAGSrrA-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Define RMSNorm Class"
      ],
      "metadata": {
        "id": "Q7OaXcEdvrkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "  \"\"\"Root Mean Square Layer Norm (no learning weights) \"\"\"\n",
        "  def __init__(self,d_model,eps:float = 1e-8):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self,x):\n",
        "    # x: (batch,d_model)\n",
        "    rms = torch.sqrt(x.pow(2).mean(dim=-1,keepdim=True)+ self.eps)\n",
        "    return x / rms"
      ],
      "metadata": {
        "id": "7brrcEzBvrYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Define the Multi-Token Prediction (MTP) class"
      ],
      "metadata": {
        "id": "-2jlelWbzk5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMTP(nn.Module):\n",
        "  def __init__(self,d_model:int,vocab_size:int,num_heads:int=3,nhead: int =1):\n",
        "    \"\"\"\n",
        "    d_model: hidden size (8 in this example)\n",
        "    num_heads: number of sequential MTP steps (D)\n",
        "    nhead: attention heads in each Transformer block\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    # shared modules\n",
        "    self.rmsnorm = RMSNorm(d_model)\n",
        "    self.embed = nn.Embedding(vocab_size,d_model)\n",
        "    self.unembed = nn.Linear(d_model,vocab_size,bias=False)\n",
        "    # share weights between embed and unembed\n",
        "    self.unembed.weight = self.embed.weight\n",
        "\n",
        "    # one projection + one Transformer per head\n",
        "    self.projections = nn.ModuleList([\n",
        "        nn.Linear(2*d_model,d_model) for _ in range(num_heads)\n",
        "\n",
        "    ])\n",
        "    self.transformers = nn.ModuleList([\n",
        "        nn.TransformerEncoderLayer(d_model=d_model,nhead=nhead)\n",
        "        for _ in range(num_heads)\n",
        "    ])\n",
        "\n",
        "  def forward(self,token_ids:torch.LongTensor,init_hidden:torch.Tensor = None):\n",
        "    \"\"\"\n",
        "    token_ids: (batch,seq_len) integer IDs of your input tokens\n",
        "    init_hidden: optional (batch,seq_len,d_model) base hidden states;\n",
        "                 if None, uses token embedding as initial hidden.\n",
        "\n",
        "    Returns:\n",
        "      logits_out: Tensor of shape (batch,T-D,D,vocab_size),\n",
        "                  where T=seq_len and D=num_heads\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    B,T = token_ids.shape\n",
        "    device = token_ids.device\n",
        "    # token embeddings: (B,T,d_model)\n",
        "    embeds = self.embed(token_ids)\n",
        "\n",
        "\n",
        "    # base hidden states\n",
        "    if init_hidden is None:\n",
        "      h0_seq = embeds           # use embeddings as base hidden\n",
        "    else:\n",
        "      h0_seq = init_hidden    # user-provided base states\n",
        "\n",
        "    outputs = [] # will hold (B,D,vocab_size) for each i\n",
        "    # slide over positions where i + D < T\n",
        "    max_i = T - self.num_heads - 1\n",
        "    for i in range(0,max_i + 1):\n",
        "      # previous hidden for depth 0 at pos i\n",
        "      h_prev = h0_seq[:,i,:] # (B,d_model)\n",
        "\n",
        "\n",
        "      # collect logits for all k at this i\n",
        "\n",
        "      logits_k = []\n",
        "\n",
        "      for k in range(self.num_heads):\n",
        "        # future token embed at pos i + (k+ 1)\n",
        "        future_pos = i + (k+1)\n",
        "        tok_embed = embeds[:,future_pos,:] # (B,d_model)\n",
        "\n",
        "        # 1) RMS-normalize\n",
        "        h_norm = self.rmsnorm(h_prev) # (B,d_model)\n",
        "        e_norm = self.rmsnorm(tok_embed) # (B,d_model)\n",
        "\n",
        "        # 2) concatenate -> (B,2*d_model)\n",
        "        merged = torch.cat([h_norm,e_norm],dim=-1)\n",
        "\n",
        "        # 3) project back to d_model\n",
        "        proj = self.projections[k](merged) # (B, d_model)\n",
        "\n",
        "        # 4) Transformer block (expects shape (S,B,d_model))\n",
        "        x = proj.unsqueeze(0)    # (1,B,d_model)\n",
        "        x = self.transformers[k](x)  # (1,B,d_model)\n",
        "        h_curr = x.squeeze(0)  # (B,d_model)\n",
        "\n",
        "        # 5) unembed -> logits\n",
        "        logits = self.unembed(h_curr)   # (B,vocab_size)\n",
        "        logits_k.append(logits)\n",
        "\n",
        "        # 6) chain hidden for next depth\n",
        "        h_prev = h_curr\n",
        "\n",
        "      # stack along. depth axis -> (B,D,vocab_size)\n",
        "      logits_k = torch.stack(logits_k,dim=1)\n",
        "      outputs.append(logits_k)\n",
        "\n",
        "    # stack along sequence axis -> (T-D,B,D,V) then permute -> (B,T-D,D,V)\n",
        "\n",
        "    out = torch.stack(outputs,dim=0)\n",
        "    out = out.permute(1,0,2,3).contiguous()\n",
        "    return out"
      ],
      "metadata": {
        "id": "y52_ypdIvq-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Pass input tokens through  the model and generate multiple next tokens."
      ],
      "metadata": {
        "id": "cxMqwj0w7GdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, seq_len,d_model,vocab_size = 1,8,8,5000\n",
        "model = SimpleMTP(d_model=d_model,vocab_size=vocab_size,num_heads=3)\n",
        "tokens = torch.randint(0,vocab_size,(batch_size,seq_len))\n",
        "\n",
        "\n",
        "# Forward pass\n",
        "logits = model(tokens)\n",
        "# logits.shape == (1,4-3,3,5000) -> (batch_size,T-D,D,V)\n",
        "print(\"Logits shape:\",logits.shape)\n",
        "\n",
        "# If you want to inspect the 1-step ahead predition at postition i=0:\n",
        "print(\"Head k=0 at i=0 logits:\",logits[0,0,0]) # a tensor of length vocab_size\n",
        "\n",
        "# Or to get all predictions at i=0 as token IDs:\n",
        "\n",
        "pred_ids = logits[0,0].argmax(dim=-1)\n",
        "print(\"Predicted tokens at i=0 for all heads:\",pred_ids)  # a length-3 tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhOmCR8X1yei",
        "outputId": "c8d9a829-b5b8-4236-8fd3-a4354059690b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([1, 5, 3, 5000])\n",
            "Head k=0 at i=0 logits: tensor([ 3.6052,  2.8964, -1.7114,  ...,  1.4961, -3.3179,  1.1599],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Predicted tokens at i=0 for all heads: tensor([4207, 4708, 4765])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Calcuate loss betweeen Loss between target tokens and predicted tokens"
      ],
      "metadata": {
        "id": "9GP9CvKg-VOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, seq_len, vocab_size = 1,8,5000\n",
        "\n",
        "# old (wrong): targets = torch.randint(0, vocab_size,(1,4))\n",
        "# new (right):\n",
        "\n",
        "targets = torch.randint(0,vocab_size,(batch_size,seq_len))\n",
        "print(\"targets.shape ->\",targets.shape) # torch.Size([1,8])\n",
        "\n",
        "\n",
        "# Now recompute:\n",
        "\n",
        "logits = model(tokens)   # shape (1,5,3,5000)\n",
        "B,L,D,V = logits.shape    # (1,5,3,5000)\n",
        "_,T = targets.shape  # (1,8)\n",
        "assert L == T - D     # 5 == 8 -3 passes\n",
        "\n",
        "\n",
        "# Double-loop loss:\n",
        "loss = 0.0\n",
        "for i in range(L):\n",
        "  for k in range(D):    # i = 0...4\n",
        "    logits_ik = logits[:,i,k,:]   # (1,5000)\n",
        "    target_ik = targets[:,i + (k + 1)] # (1,)\n",
        "    loss += F.cross_entropy(logits_ik,target_ik)\n",
        "\n",
        "loss = loss / (L*D)\n",
        "print(\"MTP loss:\",loss.item())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fvZFMQy-yXk",
        "outputId": "ff4b3e52-7d7a-4cb6-f1c0-12ef0f800f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "targets.shape -> torch.Size([1, 8])\n",
            "MTP loss: 13.472195625305176\n"
          ]
        }
      ]
    }
  ]
}